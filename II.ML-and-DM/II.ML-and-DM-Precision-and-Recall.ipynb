{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Precision and Recall\n",
    "\n",
    "by \n",
    "\n",
    "[__Michael Granitzer__ (michael.granitzer@uni-passau.de)]( http://www.mendeley.com/profiles/michael-granitzer/)\n",
    "\n",
    "\n",
    "__License__\n",
    "\n",
    "This work is licensded under a [Creative Commons Attribution 3.0 Unported License](http://creativecommons.org/licenses/by/3.0/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we introduce precision and recall as evaluation measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Class Contingency Table\n",
    "\n",
    "Let  $f_c(x)\\Rightarrow\\{0,1\\}$ be a classifier that that assigns an instance $x$ to a class $c$ if $f_c(x)=1$. Furthermore, let $g(x)=1$ ($g(x)=0$) if $x$ belongs (does not belong) to class $c$.\n",
    "\n",
    "Then, the contingencey table $T_c$ is given as\n",
    "\n",
    "|$c$      | $f_c(x)=1$    | $f_c(x)=0$    |\n",
    "|:------------:|--------------|-----------|\n",
    "|$g(x)=1$    | $TP_c$     | $FN_c$    | \n",
    "|$g(x)=0$    | $FP_c$     | $TN_c$    | \n",
    "\n",
    "We can calculate precision as\n",
    "\n",
    "$$\n",
    "\\pi_c = \\frac{TP_c}{TP_c+FP_c}\n",
    "$$\n",
    "\n",
    "and recall as\n",
    "\n",
    "$$\n",
    "\\rho_c = \\frac{TP_c}{TP_c+FN_c}\n",
    "$$\n",
    "\n",
    "Combined we calculate the $F_1$ score as harmonic mean of precision and recall:\n",
    "\n",
    "$$\n",
    "F_1=\\frac{2}{\\frac{1}{\\pi_c}+\\frac{1}{\\rho_c}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T20:12:27.900356",
     "start_time": "2016-11-18T20:12:27.893237"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision  1.0 , Recall  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# we denote f as array having f[i]=1 if the i-th instance is assigned to a class and f[i]=0 otherwise\n",
    "f=np.array([0,0,0,1,1,1])\n",
    "# we use the same format for g\n",
    "g=np.array([0,0,0,1,1,1])\n",
    "# now lets compute precision and recall\n",
    "TP_c = np.logical_and(f==1,g==1).sum()\n",
    "FP_c = np.logical_and(f==1,g==0).sum()\n",
    "FN_c = np.logical_and(f==0,g==1).sum()\n",
    "print(\"Precision \", TP_c/(TP_c+FP_c), \", Recall \", TP_c/(TP_c+FN_c),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:04:02.363067",
     "start_time": "2016-11-18T21:04:02.350595"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now lets put this into a function and show some results and cover edge cases\n",
    "def binary_contingency_table(g, f):\n",
    "    TP_c = np.logical_and(f==1,g==1).sum()\n",
    "    FP_c = np.logical_and(f==1,g==0).sum()\n",
    "    FN_c = np.logical_and(f==0,g==1).sum()\n",
    "    return (TP_c, FP_c, FN_c)\n",
    "\n",
    "def binary_precision_recall(g, f):\n",
    "    TP_c, FP_c, FN_c = binary_contingency_table(g, f)\n",
    "    if TP_c == 0: \n",
    "      return (.0, .0)\n",
    "    else:\n",
    "      return (TP_c/(TP_c + FP_c),\n",
    "              TP_c/(TP_c + FN_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:04:03.781306",
     "start_time": "2016-11-18T21:04:03.775989"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fully correct\n",
    "binary_precision_recall (np.array([0,0,0,1,1,1]), np.array([0,0,0,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:04:04.459348",
     "start_time": "2016-11-18T21:04:04.452784"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trivial acceptor\n",
    "binary_precision_recall (np.array([1,1,1,1,1,1]), np.array([0,0,0,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:04:05.881257",
     "start_time": "2016-11-18T21:04:05.876399"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trivial rejector\n",
    "binary_precision_recall (np.array([0,0,0,0,0,0]), np.array([0,0,0,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T20:26:24.798218",
     "start_time": "2016-11-18T20:26:24.795505"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can also use sklearn metrics package\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T20:28:04.980897",
     "start_time": "2016-11-18T20:28:04.967312"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.66666666666666663, 0.5, 0.57142857142857151, None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(np.array([0,1,1,1,0,1]), \n",
    "                                np.array([0,0,0,1,1,1]), \n",
    "                                average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for `sklearn` we had to select `average=\"binary\"` to indicate that we only want to take the positive class, i.e. test on \"1\". If we do not use that option, sklearn will create two contingency tables, one for class 0 and one for class 1 and calculate precision recall regarding those two classes (see multi-class below for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T20:27:54.107661",
     "start_time": "2016-11-18T20:27:54.100541"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.66666666666666663, 0.5, 0.57142857142857151, None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(np.array([0,1,1,1,0,1]), \n",
    "                                np.array([0,0,0,1,1,1]), \n",
    "                                average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "How do we calculate precision and recall when we have more classes?\n",
    "\n",
    "Let $C$ be a set of $k$ classes, i.e. $C=\\{0,1, \\ldots, k\\}$ and $C\\subseteq\\mathbf{N}$. Let  $f(x)\\Rightarrow\\{0,1, \\ldots, k\\}$ be a classifier that that assigns an instance $x$ to class $c \\in C$ if $f(x)=c$. Furthermore, let $g(x)=c$ if $x$ belongs to class $c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:16:14.091293",
     "start_time": "2016-11-18T21:16:14.076198"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class  1  precision  1.0  recall  1.0\n",
      "class  2  precision  1.0  recall  0.666666666667\n",
      "class  3  precision  0.5  recall  1.0\n",
      "Macroaverage Precision  0.833333333333 , Macroaverage Recall  0.888888888889\n",
      "class  1  tp= 2  fp= 0  fn= 0\n",
      "class  2  tp= 2  fp= 1  fn= 0\n",
      "class  3  tp= 1  fp= 0  fn= 1\n",
      "Microaverage precision  0.833333333333 , Microaverage recall  0.833333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# we denote f as array having f[i]=1 if the i-th instance is assigned to a class and f[i]=0 otherwise\n",
    "f=np.array([1,2,3,1,2,3])\n",
    "# we use the same format for g\n",
    "g=np.array([1,2,3,1,2,3])\n",
    "# classes\n",
    "c = np.unique(g)\n",
    "# now lets compute precision and recall by averaging over classes\n",
    "p_, r_ = .0, .0\n",
    "for i in c:\n",
    "    p,r = precision_recall(g==i,f==i)\n",
    "    p_ += p\n",
    "    r_ += r\n",
    "    print (\"class \",i, \" precision \", p, \" recall \", r)\n",
    "print (\"Macroaverage Precision \",p_/len(c), \", Macroaverage Recall \",r_/len(c))\n",
    "# now lets compute precision and recall by averaging over instances\n",
    "tp, fp, fn = 0, 0, 0\n",
    "for i in c:\n",
    "    TP_i, FP_i, FN_i = binary_contingency_table(g==i,f==i)\n",
    "    print(\"class \",i, \" tp=\", TP_i, \" fp=\", FP_i, \" fn=\", FN_i)\n",
    "    tp += TP_i\n",
    "    fp += FP_i\n",
    "    fn += FN_i\n",
    "print (\"Microaverage precision \", tp/(tp+fp), \", Microaverage recall \", tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:20:09.825148",
     "start_time": "2016-11-18T21:20:09.818963"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro: (0.88888888888888884, 0.83333333333333337, 0.8222222222222223, None)\n",
      "Micro: (0.83333333333333337, 0.83333333333333337, 0.83333333333333337, None)\n"
     ]
    }
   ],
   "source": [
    "g, f = np.array([1,2,3,1,2,3]), np.array([1,2,2,1,2,3])\n",
    "print(\"Macro:\",precision_recall_fscore_support(g,f, average='macro'))\n",
    "print(\"Micro:\",precision_recall_fscore_support(g,f, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class, Multi-Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:37:22.562066",
     "start_time": "2016-11-18T21:37:22.544597"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro: (1.0, 0.72222222222222221, 0.8222222222222223, None)\n",
      "Micro: (1.0, 0.66666666666666663, 0.80000000000000004, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import scipy.sparse as sp\n",
    "classes = np.array([1,2,3])\n",
    "# ground truth and prediction in the format of list of instances with a tuple of assigned classes for every instance\n",
    "g, f = np.array([(1,2),(1,2), (2, ), (3,)]), np.array([(1,),(2,), (2, ), (3,)])\n",
    "sm_g = sp.dok_matrix((len(g),len(classes)))\n",
    "for instance_ix, classes_ix in enumerate(g):\n",
    "    for clasz in classes_ix:\n",
    "        sm_g[instance_ix, clasz-1]=1\n",
    "sm_f = sp.dok_matrix((len(f),len(classes)))       \n",
    "for instance_ix, classes_ix in enumerate(f):\n",
    "    for clasz in classes_ix:\n",
    "        sm_f[instance_ix, clasz-1]=1\n",
    "    \n",
    "print(\"Macro:\",precision_recall_fscore_support(sm_g,sm_f, average='macro'))\n",
    "print(\"Micro:\",precision_recall_fscore_support(sm_g,sm_f, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T21:27:56.811049",
     "start_time": "2016-11-18T21:27:56.799178"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This class is not intended to be instantiated directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-d6865446d873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, maxprint)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'spm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             raise ValueError(\"This class is not intended\"\n\u001b[0m\u001b[1;32m     74\u001b[0m                             \" to be instantiated directly.\")\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This class is not intended to be instantiated directly."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- Show that accuracy is the same as microaveraged-precision and microaveraged-recall in the binary case\n",
    "- Show that macroaverage and microaverage is the same given a equally class-distributed data set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
