{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:fd78797298ce2e78b65169d28ef6ec196a8f4e41cf51074852357730915120d3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Machine Learning in a Nutshell with scikit-learn\n",
      "    \n",
      "## Overview and Preprocessing\n",
      "\n",
      "\n",
      "by \n",
      "\n",
      "[__Michael Granitzer__ (michael.granitzer@uni-passau.de)]( http://www.mendeley.com/profiles/michael-granitzer/)\n",
      "\n",
      "with examples taken from the scikit-learn documentation under http://scikit-learn.org/stable/\n",
      "\n",
      "\n",
      "__License__\n",
      "\n",
      "This work is licensed under a [Creative Commons Attribution 3.0 Unported License](http://creativecommons.org/licenses/by/3.0/)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Outline\n",
      "\n",
      "0. What is scikit-learn (sklearn)\n",
      "1. Scikit Learning and Machine Learning Overview\n",
      "2. Data Preprocessing\n",
      "\n",
      "\n",
      "You may also refer to the [scikit-learning Introduction](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_number": 3
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\n",
      "scikit-learn is a Machine Learning library in Python (http://scikit-learn.org/stable/).\n",
      "\n",
      "\n",
      "* Simple and efficient tools for data mining and data analysis\n",
      "* Accessible to everybody, and reusable in various contexts\n",
      "* Built on NumPy, SciPy, and matplotlib\n",
      "* Open source, commercially usable - BSD license\n",
      "\n",
      "(see [Homepage](http://scikit-learn.org/stable/))\n",
      "\n",
      "Note that we will use scikit-learn and its short form sklearn synonymously."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 4,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 4,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Overview on Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 6,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\n",
      "<div class=\"alert alert-warning\">\n",
      "A computer program is said to learn from experience with respect to some class of tasks and a performance measure,\n",
      "if its performance at the tasks improves with the experience (Mitchel 1997).\n",
      "</div>\n",
      "\n",
      "<p/>\n",
      "<div class=\"alert alert-warning\">\n",
      "\n",
      "Machine Learning creates **formal models** from **observations**. The **models** are used to predict properties of **future observations** or **aggregate and describe** observations.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 6,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "**Example**\n",
      "\n",
      "Lets assume we have collected a data set about cars:\n",
      "\n",
      "|Customer Group| Model | Mileage | Power | Price |\n",
      "|-|-|-|-|-|\n",
      "|Family| Renault Scienic | 50,000 | 132 | 5,000|\n",
      "|Upper Class | Porsche Carrera | 10,000 | 332 | 50,000|\n",
      "|Family | Touran  | 80,000 | 90 | 15,000|\n",
      "|$\\ldots$| $\\ldots$  | $\\ldots$| $\\ldots$ | $\\ldots$|\n",
      "|?| Wonder Car| 500 | 4000 | ?|\n",
      "\n",
      "- Given a large set of cars, can we group together cards with similar price, power and mileage?\n",
      "- Can we predict the price of a new car given mileage and power?\n",
      "- Can we predict the customer group?\n",
      "- What kind of cars do upper class people drive?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 6,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Real World Applications\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 9,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Image Analysis: Face Recognition, Object Detection, Self-driving Cars\n",
      "- Text Analysis: POS Tagging, Named Entity Recognition, Speech-to-Text Analysis\n",
      "- Robotics\n",
      "- E-Commerce\n",
      "- Information Retrieval and Recommender Systems\n",
      "- $\\ldots$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 9,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "General Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 9,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "\n",
      "\n",
      "The usual machine learning setup is:\n",
      "\n",
      "1. **$n$ data samples** (e.g. $n$ cars), representing the past experience\n",
      "2. Every data sample is described by a **set of d features/attributes** (e.g. horsepower and price of the car)\n",
      "\n",
      "|$Attribut_1$|$Attribut_2$|$\\ldots$|$Attribut_d$|\n",
      "|-|-|-|-|\n",
      "|$Attribut_1$ of $Example_1$|$Attribut_2$ of $Example_1$ |$\\ldots$|$Attribut_d$ of $Example_1$|\n",
      "|$Attribut_1$ of $Example_2$|$Attribut_2$ of $Example_2$ |$\\ldots$|$Attribut_d$ of $Example_2$|\n",
      "|$\\ldots$|$\\ldots$|$\\ldots$|$\\ldots$|\n",
      "|$Attribut_d$ of $Example_n$|$Attribut_2$ of $Example_n$ |$\\ldots$|$Attribut_d$ of $Example_n$|\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 9,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Machine learning estimates a **model** (also called hypothesis) that **'best' fits the data**. Fitting means the model\n",
      "\n",
      "1. **predicts** features of yet unkown examples (e.g. predict the class a flower belongs to)\n",
      "2. **describes** properties of the examples (e.g. points belonging togehter)\n",
      "\n",
      "Building such a model is called learning, training or model fitting.\n",
      "\n",
      "Using such a model is often call \"testing\", \"model estimation\" or \"inference step\".\n",
      "\n",
      "Converting data into the necessary format for learning and testing is called **preprocessing**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 9,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Preprocessing\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 14,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Refers to the task to create and prepare the data to be consumed by the learning algorithm. Usually, the target format is an matrix holding the preprocessed data. Sklearn uses numpy for representing data.\n",
      "\n",
      "\n",
      "Preprocessing steps can be summarized as follows:\n",
      "\n",
      "1. **Feature Extraction/Integration**: Convert data into matrix or integrate different data sources into one matrix\n",
      "2. **Feature Manipulation**: Manipulate and reorganise the features of a matrix\n",
      "    * *Feature Weighting/Scaling*: Convert the range of feature values\n",
      "    * *Feature Selection*: Removing unnecessary or low quality features\n",
      "    * *Feature Transformation (Dimensionality Reduction)*: merge or combine existing features to create new features   \n",
      "    \n",
      "3. **Dataset Manipulation**: Manipulate/eliminate data points\n",
      "    * *Subsampling*: Reduce the amount of data points in case the data set is to large (Squashing)\n",
      "    * *Outlier Detection*: Remove data sets that do not fit to the data distribution\n",
      "             \n",
      "<p>\n",
      "<div class=\"alert alert-info\">\n",
      "**Feature Engineering**, the task of creating features from real world data, is the most important and time consuming step (when you apply machine learning techniques)\n",
      "</div>\n",
      "\n",
      "See http://scikit-learn.org/stable/data_transforms.html for details on preprocessing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 14,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Loading/Creating standard data sets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 14,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "sklearn offers functions for loading \n",
      "- selected standard data sets\n",
      "- loading standard data set formats, in particular the `svm_light` format\n",
      "- generating artifical data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 14,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Loading Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.datasets as ds\n",
      "boston = ds.load_boston()\n",
      "print boston.DESCR"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 14
      },
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Boston House Prices dataset\n",
        "\n",
        "Notes\n",
        "------\n",
        "Data Set Characteristics:  \n",
        "\n",
        "    :Number of Instances: 506 \n",
        "\n",
        "    :Number of Attributes: 13 numeric/categorical predictive\n",
        "    \n",
        "    :Median Value (attribute 14) is usually the target\n",
        "\n",
        "    :Attribute Information (in order):\n",
        "        - CRIM     per capita crime rate by town\n",
        "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "        - INDUS    proportion of non-retail business acres per town\n",
        "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "        - NOX      nitric oxides concentration (parts per 10 million)\n",
        "        - RM       average number of rooms per dwelling\n",
        "        - AGE      proportion of owner-occupied units built prior to 1940\n",
        "        - DIS      weighted distances to five Boston employment centres\n",
        "        - RAD      index of accessibility to radial highways\n",
        "        - TAX      full-value property-tax rate per $10,000\n",
        "        - PTRATIO  pupil-teacher ratio by town\n",
        "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "        - LSTAT    % lower status of the population\n",
        "        - MEDV     Median value of owner-occupied homes in $1000's\n",
        "\n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
        "\n",
        "This is a copy of UCI ML housing dataset.\n",
        "http://archive.ics.uci.edu/ml/datasets/Housing\n",
        "\n",
        "\n",
        "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
        "\n",
        "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
        "prices and the demand for clean air', J. Environ. Economics & Management,\n",
        "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
        "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
        "pages 244-261 of the latter.\n",
        "\n",
        "The Boston house-price data has been used in many machine learning papers that address regression\n",
        "problems.   \n",
        "     \n",
        "**References**\n",
        "\n",
        "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
        "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
        "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Data Shape:\", boston.data.shape\n",
      "print \"Features:\", boston.feature_names\n",
      "print \"Labels:\", boston.target.shape"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 14,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Data Shape: (506, 13)\n",
        "Features: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
        " 'B' 'LSTAT']\n",
        "Labels: (506,)\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 14,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "SVM-Light Data Format"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 21,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The SVM-Light Data Format is a defacto standard for storing and loading sparse data matrices in a text format. \n",
      "\n",
      "The format is\n",
      "\n",
      "    Label attribute_num:attribute_value attribute_num:attribute_value ..\n",
      "    Label attribute_num:attribute_value attribute_num:attribute_value ..\n",
      "    \n",
      "where every data points is one line\n",
      "\n",
      "\n",
      "\n",
      "- Data sets in that format can be found under http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n",
      "- A faster API compatible implementation can be found under  https://github.com/mblondel/svmlight-loader"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests as r\n",
      "data = r.get(\"http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a\")\n",
      "print data.content[0:124]"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 21,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-1 3:1 11:1 14:1 19:1 39:1 42:1 55:1 64:1 67:1 73:1 75:1 76:1 80:1 83:1 \n",
        "-1 3:1 6:1 17:1 27:1 35:1 40:1 57:1 63:1 69:1 73:1 \n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = r.get(\"http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a\"\n",
      "             ,stream = True)\n",
      "X_train, y_train = ds.load_svmlight_file(data.raw)\n",
      "print type(X_train), X_train.shape\n",
      "print type(y_train), y_train.shape"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 23,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'scipy.sparse.csr.csr_matrix'> (1605, 119)\n",
        "<type 'numpy.ndarray'> (1605,)\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 23,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Sample Data Set Generators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#see the sklearn.dataset moduls with prefix make\n",
      "# make_classificaiton for example creates a n-class classification problem\n",
      "X_train, y_train = ds.make_classification(100,10)\n",
      "\n",
      "print type(X_train), X_train.shape\n",
      "print type(y_train), y_train.shape"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 25,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'numpy.ndarray'> (100, 10)\n",
        "<type 'numpy.ndarray'> (100,)\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 25,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Feature Extraction\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 25,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "<div class =\"alert alert-warning\">\n",
      "**Feature Extraction** refers to the process of extracting features out of  the raw data an represent those features in a formal, re-usable model.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 25,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Extracting Features from Dicts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 29
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "sklearn allows you to convert python dictionaries, that represent features, into Numpy arrays.\n",
      "\n",
      "For nominal data it implements a \"one-hot\" coding (e..g one Attribute that is on or off)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "measurements = [\n",
      "        {'Model': 'Renault Scienic', 'Mileage': 50000, 'Power': 132, 'Price':5000},\n",
      "        {'Model': 'Porsche Carrera', 'Mileage': 10000, 'Power': 332, 'Price':50000},\n",
      "        {'Model': 'Touran', 'Mileage': 80000, 'Power': 90, 'Price': 15000}\n",
      "        ]\n",
      "\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "vec = DictVectorizer()\n",
      "\n",
      "print vec.fit_transform(measurements).toarray()\n",
      "vec.get_feature_names()"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 30,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  5.00000000e+04   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
        "    1.32000000e+02   5.00000000e+03]\n",
        " [  1.00000000e+04   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    3.32000000e+02   5.00000000e+04]\n",
        " [  8.00000000e+04   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
        "    9.00000000e+01   1.50000000e+04]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "['Mileage',\n",
        " 'Model=Porsche Carrera',\n",
        " 'Model=Renault Scienic',\n",
        " 'Model=Touran',\n",
        " 'Power',\n",
        " 'Price']"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 30,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Text Preprocessing using sklearn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 32
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "sklearn supports several counting methods for converting text into a matrix representation. The simplest one is the count vectorizer.\n",
      "\n",
      "Vectorizers can use analyzers (to be set in the constructor), which tokenize the text. Here you can integrate tokenizers from other libraries, like for example NLTK."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer(min_df=1)\n",
      "corpus = ['This is the first document.',\n",
      "        'This is the second second document.',\n",
      "        'And the third one.',\n",
      "        'Is this the first document?']\n",
      "word_counts = vectorizer.fit_transform(corpus)\n",
      "word_counts"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 33,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "<4x9 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 19 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 33,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "[u'and',\n",
        " u'document',\n",
        " u'first',\n",
        " u'is',\n",
        " u'one',\n",
        " u'second',\n",
        " u'the',\n",
        " u'third',\n",
        " u'this']"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 33,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Feature Scaling and Weighting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 36,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "After extracting features one needs to consider the scale and/or value of a feature. Most likely, value ranges are not sufficiently prepared for subsequent machine learning. \n",
      "\n",
      "For example, raw counts of feature occurence may not provide a meaningful feature representation. In text for example, the words with the highest frequency are stopwords and hence we need to reweight the value of a feature. \n",
      "\n",
      "\n",
      "As a second example, data coming from a sensor might contain wrong measurements or the scale between two sensors might be wrong and needs to be rescaled/normalized. \n",
      "<p>\n",
      "\n",
      "<div class = \"alert alert-info\">\n",
      "When preprocessing data, always check that \n",
      "<ol>\n",
      "<li> The extracted features (i.e. the attributes/dimensions) are meaningful and represent information such that the learning task can be solved \n",
      "  <li> The value range of the features is as expected by the machine learning algorithm and has been cleaned from problematic data\n",
      "</ol>\n",
      "\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 36,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "TFIDF Weighting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 38
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "TF IDF weighting stands for \"Term Frequency vs. Inverse Document Frequency\" weighting and is mostly used for representing textual data.\n",
      "\n",
      "The weight is proportional to the frequency how often a word occurs in a text multiplied by the inverse document frequency, i.e. how many documents contain a certain text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "transformer = TfidfTransformer()\n",
      "tfidf_counts = transformer.fit_transform(word_counts)"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 39,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Document\\tfeature\\ttfidf\\tcount\"\n",
      "for row in xrange(tfidf_counts.shape[0]):\n",
      "    for col,name in enumerate(vectorizer.get_feature_names()):\n",
      "        print \"%d\\t%s\\t%f\\t%d\"%\\\n",
      "              (row,name,\n",
      "               tfidf_counts[row,col],\n",
      "               word_counts[row,col])"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 39,
       "slide_helper": "subslide_end",
       "slide_type": "subslide"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Document\tfeature\ttfidf\tcount\n",
        "0\tand\t0.000000\t0\n",
        "0\tdocument\t0.438777\t1\n",
        "0\tfirst\t0.541977\t1\n",
        "0\tis\t0.438777\t1\n",
        "0\tone\t0.000000\t0\n",
        "0\tsecond\t0.000000\t0\n",
        "0\tthe\t0.358729\t1\n",
        "0\tthird\t0.000000\t0\n",
        "0\tthis\t0.438777\t1\n",
        "1\tand\t0.000000\t0\n",
        "1\tdocument\t0.272301\t1\n",
        "1\tfirst\t0.000000\t0\n",
        "1\tis\t0.272301\t1\n",
        "1\tone\t0.000000\t0\n",
        "1\tsecond\t0.853226\t2\n",
        "1\tthe\t0.222624\t1\n",
        "1\tthird\t0.000000\t0\n",
        "1\tthis\t0.272301\t1\n",
        "2\tand\t0.552805\t1\n",
        "2\tdocument\t0.000000\t0\n",
        "2\tfirst\t0.000000\t0\n",
        "2\tis\t0.000000\t0\n",
        "2\tone\t0.552805\t1\n",
        "2\tsecond\t0.000000\t0\n",
        "2\tthe\t0.288477\t1\n",
        "2\tthird\t0.552805\t1\n",
        "2\tthis\t0.000000\t0\n",
        "3\tand\t0.000000\t0\n",
        "3\tdocument\t0.438777\t1\n",
        "3\tfirst\t0.541977\t1\n",
        "3\tis\t0.438777\t1\n",
        "3\tone\t0.000000\t0\n",
        "3\tsecond\t0.000000\t0\n",
        "3\tthe\t0.358729\t1\n",
        "3\tthird\t0.000000\t0\n",
        "3\tthis\t0.438777\t1\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 39,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Standardization (mean removal/variance scaling)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 42
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Some machine learning methods do not work well if the value range of attributes is not standardized. Standardization assume that values are normally distributed and aims at removing mean and scaling the values to unit variance.\n",
      "\n",
      "Standardization is often refered to as Feature Normalization (i.e. normalization along one attribute)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "import numpy as np\n",
      "X = np.array([[ 1., -1.,  2.],\n",
      "               [ 2.,  0.,  0.],\n",
      "               [ 0.,  1., -1.]])\n",
      "X_scaled = preprocessing.scale(X)\n",
      "print \"Mean original data\", np.mean(X, axis=0)\n",
      "print \"Var  original data\",np.var(X, axis=0)\n",
      "print \"Mean scaled   data\",np.mean(X_scaled, axis=0)\n",
      "print \"Var  scaled   data\",np.var(X_scaled, axis=0)\n",
      "                                         \n"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 43,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mean original data [ 1.          0.          0.33333333]\n",
        "Var  original data [ 0.66666667  0.66666667  1.55555556]\n",
        "Mean scaled   data [ 0.  0.  0.]\n",
        "Var  scaled   data [ 1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 43,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Min/Max Scaling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 45
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Alternatively, one can simply scale the feature range according to the minimum and maximum value in the data set such that the new feature range is in the range $[0:1]$. \n",
      "\n",
      "This is done by the `MinMaxScaler`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_max_scaler = preprocessing.MinMaxScaler()\n",
      "X_scale_minmax = min_max_scaler.fit_transform(X)\n",
      "print X_scale_minmax"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 46
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.5         0.          1.        ]\n",
        " [ 1.          0.5         0.33333333]\n",
        " [ 0.          1.          0.        ]]\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 46
      },
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1. -1.  2.]\n",
        " [ 2.  0.  0.]\n",
        " [ 0.  1. -1.]]\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#alternative calculation\n",
      "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) \n",
      "print X_std - X_scale_minmax "
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 46,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.  0.  0.]\n",
        " [ 0.  0.  0.]\n",
        " [ 0.  0.  0.]]\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 46,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Normalization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 50
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Normalization deals with the problem that data point vectors can be of very different length. Consider for example a short and a long document.\n",
      "\n",
      "Normalization brings all data points to unit length. This is necessary by methods relying on the dot product between data points."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_normalized = preprocessing.normalize(X, norm='l2')\n",
      "print np.linalg.norm(X, ord=2, axis=1)\n",
      "print np.linalg.norm(X_normalized, ord=2, axis=1)"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 51,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "subslide_end",
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 2.44948974  2.          1.41421356]\n",
        "[ 1.  1.  1.]\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 51,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Final Remarks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 53,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<div class = \"alert alert-info\">\n",
      "In practical applications, **Preprocessing** is the most cruical step in applying machine learning. It depends on the machine learning technique used afterwards, the data at hand and the skill of the feature engineer.\n",
      "<p>\n",
      "So do not underestimate this step. A good to ask is whether you, as a human, could solve the task given the information obtained from preprocessing. If you can't, the machine will, most likely, can't do it either.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false,
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 53,
       "slide_type": "subslide"
      },
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 55
      },
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Tom Mitchell, Machine Learning, McGraw-Hill 1997 [chapter slides](http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html)\n",
      "- [Scikit learn](http://scikit-learn.org/stable/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 55
      },
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "# Notebook Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Stuff for presentation\n",
      "#loads section numbering https://github.com/ipython/ipython/wiki/Extensions-Index\n",
      "%reload_ext secnum\n",
      "%secnum"
     ],
     "language": "python",
     "metadata": {
      "internals": {
       "frag_helper": "fragment_end",
       "frag_number": 55,
       "slide_helper": "subslide_end"
      },
      "slide_helper": "slide_end",
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "outputs": [
      {
       "javascript": [
        "console.log(\"Section numbering...\");\n",
        "\n",
        "function number_sections(threshold) {\n",
        "\n",
        "  var h1_number = 0;\n",
        "  var h2_number = 0;\n",
        "\n",
        "  if (threshold === undefined) {\n",
        "    threshold = 2;  // does nothing so far\n",
        "  }\n",
        "\n",
        "  var cells = IPython.notebook.get_cells();\n",
        "  \n",
        "  for (var i=0; i < cells.length; i++) {\n",
        "\n",
        "    var cell = cells[i];\n",
        "    if (cell.cell_type !== 'heading') continue;\n",
        "    \n",
        "    var level = cell.level;\n",
        "    if (level > threshold) continue;\n",
        "    \n",
        "    if (level === 1) {\n",
        "        \n",
        "        h1_number ++;\n",
        "        var h1_element = cell.element.find('h1');\n",
        "        var h1_html = h1_element.html();\n",
        "        \n",
        "        console.log(\"h1_html: \" + h1_html);\n",
        "\n",
        "        var patt = /^[0-9]+\\.\\s(.*)/;   // section number at start of string\n",
        "        var title = h1_html.match(patt);  // just the title\n",
        "\n",
        "        if (title != null) {  \n",
        "          h1_element.html(h1_number + \". \" + title[1]);\n",
        "        }\n",
        "        else {\n",
        "          h1_element.html(h1_number + \". \" + h1_html);\n",
        "        }\n",
        "        \n",
        "        h2_number = 0;\n",
        "        \n",
        "    }\n",
        "    \n",
        "    if (level === 2) {\n",
        "    \n",
        "        h2_number ++;\n",
        "        \n",
        "        var h2_element = cell.element.find('h2');\n",
        "        var h2_html = h2_element.html();\n",
        "\n",
        "        console.log(\"h2_html: \" + h2_html);\n",
        "\n",
        "        \n",
        "        var patt = /^[0-9]+\\.[0-9]+\\.\\s/;\n",
        "        var result = h2_html.match(patt);\n",
        "\n",
        "        if (result != null) {\n",
        "          h2_html = h2_html.replace(result, \"\");\n",
        "        }\n",
        "\n",
        "        h2_element.html(h1_number + \".\" + h2_number + \". \" + h2_html);\n",
        "        \n",
        "    }\n",
        "    \n",
        "  }\n",
        "  \n",
        "}\n",
        "\n",
        "number_sections();\n",
        "\n",
        "// $([IPython.evnts]).on('create.Cell', number_sections);\n",
        "\n",
        "$([IPython.events]).on('selected_cell_type_changed.Notebook', number_sections);\n",
        "\n"
       ],
       "metadata": {},
       "output_type": "display_data"
      }
     ],
     "prompt_number": 2
    }
   ],
   "metadata": {}
  }
 ]
}