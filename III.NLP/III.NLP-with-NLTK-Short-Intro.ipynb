{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:dfbfde7d87e8ed7d25ceb474c7724480ef766b80836ed695ded82476782ba8bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# A brief introdcution to Natural Language Processing with Python NLTK\n",
      "\n",
      "by \n",
      "\n",
      "[__Michael Granitzer__ (michael.granitzer@uni-passau.de)]( http://www.mendeley.com/profiles/michael-granitzer/)\n",
      "\n",
      "<p>\n",
      "<p>\n",
      "<br>\n",
      "__Licences__\n",
      "<br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
      "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\" align=\"left\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a>\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Overview - Natural Language Processing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<div class=\"alert alert-warning\">\n",
      "**Natural Language Processing** is the field concerned with the development of theories, methods and systems that allow computers to analyse, process, understand and generate natural human language. \n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Challenges in Natural Language**\n",
      "\n",
      "- Ambiguity is inherent in natural language\n",
      "- Complex, context sensitive grammatic\n",
      "- Understanding natural language requires a large amount of background knowledge \n",
      "- Natural language occurs in different modalities, i.e. written text, spoken text, which require different preprocessing\n",
      "\n",
      "**Machine Learning, Stochastic and probabilistical methods** provide central components for solving challenges in natural language processing. They are summarized under the term **statistical NLP** "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "NLP Tasks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Some NLP Tasks**\n",
      "\n",
      "- Automatic Summarization: Summarize a given text\n",
      "- Coreference resultion: Resolve references of one word to another word in text (e.g. pronouns to nouns)\n",
      "- Machine translation: Automatically translate from one language into another\n",
      "- Morphological Analysis: Separate words to morphemes and identify their class.\n",
      "- Named Entity Recognition: determine proper nouns and their class\n",
      "- Natural Language Generation\n",
      "- Natural Language Understanding: Convert text into a formal representation like first order logic\n",
      "- Optical Charachter Recognition: Convert images to text\n",
      "- Part-of-Speech tagging: Determine the part of speech for each word\n",
      "- Parsing: Conduct a grammatical analysis, i.e. extract a parsing tree\n",
      "- Question and Answering: Answer human-language questions\n",
      "- Relationship Extraction: determing relationships between named entities\n",
      "- Sentiment Analysis: analyse subjective information and determine the polarity of a text\n",
      "- Speech recognition: Converte speech to text\n",
      "- Speech segmentation: separate spoken language into words\n",
      "- Topic segmentation: separate a chunk of text into topics\n",
      "- Word segementation: separate a chunk of text into separate words (relevant for most non-Western languages)\n",
      "- Word sense disambiguation: Determine the meaning of a word.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "**Related Fields sharing some tasks**\n",
      "\n",
      "- Information Retrieval - storing, searching and retrieving (textual) information\n",
      "- Information Extraction - extracting semantic information from text\n",
      "- Speech Processing - processing and analyzing spoken language\n",
      "- Web Content Mining - Analysing web conten"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "1.2. IBM WATSON - An Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import YouTubeVideo\n",
      "YouTubeVideo('WFR3lOm_xhE')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"400\"\n",
        "            height=300\"\n",
        "            src=\"https://www.youtube.com/embed/WFR3lOm_xhE\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<IPython.lib.display.YouTubeVideo at 0x106611e10>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "NLTK - The Python Natural Language Toolkit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Python offers a rich set of libraries for natural language processing. The most famous and most complete one is the **Natural Language Toolkit** short **NLTK**. Current version is 3.0."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<div class=\"alert alert-warning\">\n",
      "\"**NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, and an active discussion forum.\" [nltk.org](http://www.nltk.org/)\n",
      "</div>\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Installation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For details see the [install instructions](http://www.nltk.org/install.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the cheap variant\n",
      "try:\n",
      "   import nltk\n",
      "except:\n",
      "   !pip install --user nltk "
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "NLTK relies on some resourcs, which have to be installed separately. nltk provides a download interface, which we will need later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#show the available corpora through the nlp download method\n",
      "nltk.download() # look for a window that is opened"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Large set of functionalities: corpus reader, tokenizers, stemmers, taggers, chunker, parsers, word net\n",
      "- rich set of integrated text corpora for testing/evaluation\n",
      "- Good documentation and sources for learning both, python and NLP ([see the NLTK Book](http://www.nltk.org/book/))"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Usage"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import the nltk module\n",
      "import nltk\n",
      "help(nltk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on package nltk:\n",
        "\n",
        "NAME\n",
        "    nltk\n",
        "\n",
        "FILE\n",
        "    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/__init__.py\n",
        "\n",
        "DESCRIPTION\n",
        "    The Natural Language Toolkit (NLTK) is an open source Python library\n",
        "    for Natural Language Processing.  A free online book is available.\n",
        "    (If you use the library for academic research, please cite the book.)\n",
        "    \n",
        "    Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
        "    Natural Language Processing with Python.  O'Reilly Media Inc.\n",
        "    http://nltk.org/book\n",
        "    \n",
        "    @version: 3.0.0\n",
        "\n",
        "PACKAGE CONTENTS\n",
        "    align (package)\n",
        "    app (package)\n",
        "    book\n",
        "    ccg (package)\n",
        "    chat (package)\n",
        "    chunk (package)\n",
        "    classify (package)\n",
        "    cluster (package)\n",
        "    collocations\n",
        "    compat\n",
        "    corpus (package)\n",
        "    data\n",
        "    decorators\n",
        "    downloader\n",
        "    draw (package)\n",
        "    featstruct\n",
        "    grammar\n",
        "    help\n",
        "    inference (package)\n",
        "    internals\n",
        "    jsontags\n",
        "    lazyimport\n",
        "    metrics (package)\n",
        "    misc (package)\n",
        "    parse (package)\n",
        "    probability\n",
        "    sem (package)\n",
        "    stem (package)\n",
        "    tag (package)\n",
        "    tbl (package)\n",
        "    test (package)\n",
        "    text\n",
        "    tokenize (package)\n",
        "    toolbox\n",
        "    tree\n",
        "    treetransforms\n",
        "    util\n",
        "    wsd\n",
        "\n",
        "SUBMODULES\n",
        "    agreement\n",
        "    api\n",
        "    association\n",
        "    boxer\n",
        "    brill\n",
        "    brill_trainer\n",
        "    chart\n",
        "    confusionmatrix\n",
        "    decisiontree\n",
        "    dependencygraph\n",
        "    discourse\n",
        "    distance\n",
        "    drt\n",
        "    earleychart\n",
        "    em\n",
        "    evaluate\n",
        "    featurechart\n",
        "    gaac\n",
        "    glue\n",
        "    hmm\n",
        "    hunpos\n",
        "    ibm1\n",
        "    ibm2\n",
        "    ibm3\n",
        "    isri\n",
        "    kmeans\n",
        "    lancaster\n",
        "    lfg\n",
        "    linearlogic\n",
        "    logic\n",
        "    mace\n",
        "    malt\n",
        "    mapping\n",
        "    maxent\n",
        "    megam\n",
        "    naivebayes\n",
        "    nltk.app\n",
        "    nltk.chat\n",
        "    nltk.corpus\n",
        "    nltk.toolbox\n",
        "    nonprojectivedependencyparser\n",
        "    paice\n",
        "    pchart\n",
        "    porter\n",
        "    positivenaivebayes\n",
        "    projectivedependencyparser\n",
        "    prover9\n",
        "    punkt\n",
        "    recursivedescent\n",
        "    regexp\n",
        "    relextract\n",
        "    resolution\n",
        "    rslp\n",
        "    rte_classify\n",
        "    scikitlearn\n",
        "    scores\n",
        "    segmentation\n",
        "    sequential\n",
        "    sexpr\n",
        "    shiftreduce\n",
        "    simple\n",
        "    snowball\n",
        "    spearman\n",
        "    stanford\n",
        "    tableau\n",
        "    tadm\n",
        "    texttiling\n",
        "    tnt\n",
        "    treebank\n",
        "    viterbi\n",
        "    weka\n",
        "    wordnet\n",
        "\n",
        "FUNCTIONS\n",
        "    demo()\n",
        "        # override any accidentally imported demo\n",
        "\n",
        "DATA\n",
        "    SLASH = *slash*\n",
        "    TYPE = *type*\n",
        "    __author__ = 'Steven Bird, Edward Loper, Ewan Klein'\n",
        "    __author_email__ = 'stevenbird1@gmail.com'\n",
        "    __classifiers__ = ['Development Status :: 5 - Production/Stable', 'Int...\n",
        "    __copyright__ = 'Copyright (C) 2001-2014 NLTK Project.\\n\\nDistribut......\n",
        "    __keywords__ = ['NLP', 'CL', 'natural language processing', 'computati...\n",
        "    __license__ = 'Apache License, Version 2.0'\n",
        "    __longdescr__ = 'The Natural Language Toolkit (NLTK) is a Python ... p...\n",
        "    __maintainer__ = 'Steven Bird, Edward Loper, Ewan Klein'\n",
        "    __maintainer_email__ = 'stevenbird1@gmail.com'\n",
        "    __url__ = 'http://nltk.org/'\n",
        "    __version__ = '3.0.0'\n",
        "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
        "    class_types = (<type 'type'>, <type 'classobj'>)\n",
        "    corpus = <LazyModule 'nltk.corpus'>\n",
        "    infile = <closed file '/opt/local/Library/Frameworks/Pyth.../python2.7...\n",
        "    json_tags = {'!nltk.tag.BrillTagger': <class 'nltk.tag.brill.BrillTagg...\n",
        "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
        "    string_types = (<type 'basestring'>,)\n",
        "    tkinter = <nltk.compat.TkinterPackage object>\n",
        "    toolbox = <LazyModule 'nltk.toolbox'>\n",
        "    version_file = '/opt/local/Library/Frameworks/Python.framework/Version...\n",
        "    version_info = sys.version_info(major=2, minor=7, micro=8, releaseleve...\n",
        "\n",
        "VERSION\n",
        "    3.0.0\n",
        "\n",
        "AUTHOR\n",
        "    Steven Bird, Edward Loper, Ewan Klein\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Processing Raw Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Usually text processing starts with an artefact containing textual information in some form (e.g. plain text, PDF Document, audio signal) and involves the following steps:\n",
      "\n",
      "1. Binary Content to Text: Convert the binary data to a string representation\n",
      "\n",
      "   `bytes --> String`\n",
      "   <p>  \n",
      "2. Tokenization: split the string into tokens\n",
      "   1. Sentence Tokenization: every token represents a sentence\n",
      "   2. Word Tokenization: Every token represents a word\n",
      "   3. Character Tokenization: Every token represents a character sequence\n",
      "\n",
      "  `String --> list of list of strings` \n",
      "  \n",
      "  or\n",
      "  \n",
      "  `String --> list of strings`\n",
      " \n",
      "3. Normalization and Tagging: \n",
      "\n",
      "   1. reduce tokens into a normalized form (e.g. lower case, word stems etc.)\n",
      "\n",
      "         `list of strings --> list of strings`\n",
      "     <p>    \n",
      "   2. tag tokens with additional information, like their part of speech\n",
      "   \n",
      "         `list of strings --> list of tuples`\n",
      "         \n",
      "4. Filtering and Vocabulary Building: We can now filter the list of tokens to our need, for example keep only nouns, and create and analyse the vocabulary.\n",
      "\n",
      "     `list of tuples or strings --> set of tuples or strings`\n",
      "     <p>\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Processing raw text example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the following example we will load text from a web site and extract tokens out of it.\n",
      "\n",
      "Note that we need the [requests module](http://docs.python-requests.org/en/latest/) for making http requests.\n",
      "\n",
      "Further, in order to parse HTML we will use the [Beautiful Soup library](http://www.crummy.com/software/BeautifulSoup/), which also needs to be installed. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    import requests\n",
      "except:\n",
      "    !easy_install --user requests\n",
      "try:\n",
      "    import bs4\n",
      "except:\n",
      "    !easy_install --user beautifulsoup4"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Step1: Binary to Text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Binary to text is the most obvious step, but most often also the most critical. Most often, binary formats do not allow to extract only they usefull text. Especially in Web Mining Cleaning the data can be very cumbesome and labour intesive."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#fetch the web page\n",
      "import requests\n",
      "url = \"http://www.nltk.org/book/ch03.html\"\n",
      "r = requests.get(url)\n",
      "r.text[0:200]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "u'<?xml version=\"1.0\" encoding=\"ascii\" ?>\\n\\n<script language=\"javascript\" type=\"text/javascript\">\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(<([^>]+)>)/ig,\"\")\\n                         .'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#clean html\n",
      "from bs4  import BeautifulSoup\n",
      "text = BeautifulSoup(r.text).get_text()\n",
      "#not perfect, but we have text now.\n",
      "print type(text)\n",
      "print text [0:500]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'unicode'>\n",
        "\n",
        "\n",
        "function astext(node)\n",
        "{\n",
        "    return node.innerHTML.replace(/(<([^>]+)>)/ig,\"\")\n",
        "                         .replace(/&gt;/ig, \">\")\n",
        "                         .replace(/&lt;/ig, \"<\")\n",
        "                         .replace(/&quot;/ig, '\"')\n",
        "                         .replace(/&amp;/ig, \"&\");\n",
        "}\n",
        "\n",
        "function copy_notify(node, bar_color, data)\n",
        "{\n",
        "    // The outer box: relative + inline positioning.\n",
        "    var box1 = document.createElement(\"div\");\n",
        "    box1.style.position = \"relative\";\n",
        "    box1.style.display = \"inline\";\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "soup = BeautifulSoup(r.text)\n",
      "print soup.prettify()[0:500]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<html>\n",
        " <head>\n",
        "  <script language=\"javascript\" type=\"text/javascript\">\n",
        "   function astext(node)\n",
        "{\n",
        "    return node.innerHTML.replace(/(<([^>]+)>)/ig,\"\")\n",
        "                         .replace(/&gt;/ig, \">\")\n",
        "                         .replace(/&lt;/ig, \"<\")\n",
        "                         .replace(/&quot;/ig, '\"')\n",
        "                         .replace(/&amp;/ig, \"&\");\n",
        "}\n",
        "\n",
        "function copy_notify(node, bar_color, data)\n",
        "{\n",
        "    // The outer box: relative + inline positioning.\n",
        "    var box1 = document.createElement(\"div\");\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Lets remove script and style tags\n",
      "soup = BeautifulSoup(r.text)\n",
      "[s.extract() for s in soup(['script','style', 'xml', 'head'])]\n",
      "text = soup.get_text()\n",
      "print text[0:500]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ".. -*- mode:\n",
        "System Message: WARNING/2 (ch03.rst2, line 1); backlink\n",
        "Inline emphasis start-string without end-string.\n",
        "rst -*-\n",
        "\n",
        "System Message: WARNING/2 (ch03.rst2, line 1); backlink\n",
        "Inline emphasis start-string without end-string.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "System Message: WARNING/2 (ch03.rst2, line 2)\n",
        "Field list ends without a blank line; unexpected unindent.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3\u00a0\u00a0\u00a0Processing Raw Text\n",
        "The most important source of texts is undoubtedly the Web.  It's convenient\n",
        "to h\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "As you can see, we got rid of the HTML tags. However, this does not necessarily mean we have the correct content. Usually a more intelligent approach is needed to extract the interesting content from web pages. However, this is beyond the scope of this lecture."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Step 2: Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tokenization can work on different textual elements and using different techniques. In principal one can distinguish between\n",
      "\n",
      "- word vs. sentence level tokenization\n",
      "- rule-based vs. machine learning based tokenizers\n",
      "\n",
      "Although tokenization seems to be pretty simple, a number of disambiguities can arise, like for example through ambigous meaning of punctuations (e.g. \".\" in \"12.12.2016\", \"didn't\").\n",
      "\n",
      "NLTK contains state of the art models providing good results.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "help(nltk.sent_tokenize)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function sent_tokenize in module nltk.tokenize:\n",
        "\n",
        "sent_tokenize(text)\n",
        "    Return a sentence-tokenized copy of *text*,\n",
        "    using NLTK's recommended sentence tokenizer\n",
        "    (currently :class:`.PunktSentenceTokenizer`).\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#note that this requires you to install the \"punkt\" module by invoking nltk.download()\n",
      "sent = nltk.sent_tokenize(text)\n",
      "print \"Found %d sentences\"%len(sent)\n",
      "print \"E.g. sentence #22 '\",sent[22],\"'\""
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found 801 sentences\n",
        "E.g. sentence #22 ' For our language\n",
        "processing, we want to break up the string into\n",
        "words and punctuation, as we saw in 1.. '\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(nltk.word_tokenize)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function word_tokenize in module nltk.tokenize:\n",
        "\n",
        "word_tokenize(text)\n",
        "    Return a tokenized copy of *text*,\n",
        "    using NLTK's recommended word tokenizer\n",
        "    (currently :class:`.TreebankWordTokenizer`\n",
        "    along with :class:`.PunktSentenceTokenizer`).\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(text)\n",
      "for t in tokens[:20]:\n",
      "    print t"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "..\n",
        "-*-\n",
        "mode\n",
        ":\n",
        "System\n",
        "Message\n",
        ":\n",
        "WARNING/2\n",
        "(\n",
        "ch03.rst2\n",
        ",\n",
        "line\n",
        "1\n",
        ")\n",
        ";\n",
        "backlink\n",
        "Inline\n",
        "emphasis\n",
        "start-string\n",
        "without\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Step 3a: Normalization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Normalization and Tagging involves a number of different steps that aim to reduce the ambiguity of text and make it more usefull for subsequent applications. Steps include (but are not limited too)\n",
      "\n",
      "**Normalization**\n",
      "\n",
      "- Case folding - lower/upper-case characters should not distinguish words\n",
      "- Stemming - reducing words to their word form by usually stripping away affixes (e.g. plural 's' or 'ing' in english). Affixes are morphemes be at the beginning, middle and end of a word.\n",
      "- Lemmatisation - process of grouping togehter different inflected forms of a word to their common root aka lemma. Different to stemming, lemmatisation takes the context/meaning of a word into account.\n",
      "- Orhtographic representation of non-standard words - convert every number to 0.0 and every acronym to AAA for keeping the vocabulary small\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#case folding\n",
      "tokens_folded = [t.lower() for t in tokens]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Stemming using the well-known Porter Stemmer which is a rule-based affix stripper.\n",
      "#Stemming is language dependent and hence the Porter STemmer only works for english\n",
      "porter = nltk.PorterStemmer()\n",
      "stemmed = [porter.stem(t) for t in tokens]\n",
      "print \"Stemmed\\t\\t\\tOriginal\",t\n",
      "for s,t in zip(stemmed[100:200],tokens[100:200]):\n",
      "    print s,\"\\t\\t\\t\",t"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Stemmed\t\t\tOriginal .\n",
        "Howev \t\t\tHowever\n",
        ", \t\t\t,\n",
        "you \t\t\tyou\n",
        "probabl \t\t\tprobably\n",
        "have \t\t\thave\n",
        "your \t\t\tyour\n",
        "own \t\t\town\n",
        "text \t\t\ttext\n",
        "sourc \t\t\tsources\n",
        "in \t\t\tin\n",
        "mind \t\t\tmind\n",
        ", \t\t\t,\n",
        "and \t\t\tand\n",
        "need \t\t\tneed\n",
        "to \t\t\tto\n",
        "learn \t\t\tlearn\n",
        "how \t\t\thow\n",
        "to \t\t\tto\n",
        "access \t\t\taccess\n",
        "them \t\t\tthem\n",
        ". \t\t\t.\n",
        "The \t\t\tThe\n",
        "goal \t\t\tgoal\n",
        "of \t\t\tof\n",
        "thi \t\t\tthis\n",
        "chapter \t\t\tchapter\n",
        "is \t\t\tis\n",
        "to \t\t\tto\n",
        "answer \t\t\tanswer\n",
        "the \t\t\tthe\n",
        "follow \t\t\tfollowing\n",
        "question \t\t\tquestions\n",
        ": \t\t\t:\n",
        "How \t\t\tHow\n",
        "can \t\t\tcan\n",
        "we \t\t\twe\n",
        "write \t\t\twrite\n",
        "program \t\t\tprograms\n",
        "to \t\t\tto\n",
        "access \t\t\taccess\n",
        "text \t\t\ttext\n",
        "from \t\t\tfrom\n",
        "local \t\t\tlocal\n",
        "file \t\t\tfiles\n",
        "and \t\t\tand\n",
        "from \t\t\tfrom\n",
        "the \t\t\tthe\n",
        "web \t\t\tweb\n",
        ", \t\t\t,\n",
        "in \t\t\tin\n",
        "order \t\t\torder\n",
        "to \t\t\tto\n",
        "get \t\t\tget\n",
        "hold \t\t\thold\n",
        "of \t\t\tof\n",
        "an \t\t\tan\n",
        "unlimit \t\t\tunlimited\n",
        "rang \t\t\trange\n",
        "of \t\t\tof\n",
        "languag \t\t\tlanguage\n",
        "materi \t\t\tmaterial\n",
        "? \t\t\t?\n",
        "How \t\t\tHow\n",
        "can \t\t\tcan\n",
        "we \t\t\twe\n",
        "split \t\t\tsplit\n",
        "document \t\t\tdocuments\n",
        "up \t\t\tup\n",
        "into \t\t\tinto\n",
        "individu \t\t\tindividual\n",
        "word \t\t\twords\n",
        "and \t\t\tand\n",
        "punctuat \t\t\tpunctuation\n",
        "symbol \t\t\tsymbols\n",
        ", \t\t\t,\n",
        "so \t\t\tso\n",
        "we \t\t\twe\n",
        "can \t\t\tcan\n",
        "carri \t\t\tcarry\n",
        "out \t\t\tout\n",
        "the \t\t\tthe\n",
        "same \t\t\tsame\n",
        "kind \t\t\tkinds\n",
        "of \t\t\tof\n",
        "analysi \t\t\tanalysis\n",
        "we \t\t\twe\n",
        "did \t\t\tdid\n",
        "with \t\t\twith\n",
        "text \t\t\ttext\n",
        "corpora \t\t\tcorpora\n",
        "in \t\t\tin\n",
        "earlier \t\t\tearlier\n",
        "chapter \t\t\tchapters\n",
        "? \t\t\t?\n",
        "How \t\t\tHow\n",
        "can \t\t\tcan\n",
        "we \t\t\twe\n",
        "write \t\t\twrite\n",
        "program \t\t\tprograms\n",
        "to \t\t\tto\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Lemmatization usually requires some linguistic resource that contains a list of lemmas\n",
      "#again language dependent.\n",
      "#WordNet is a thesaurus providing this kind of information.\n",
      "wnl = nltk.WordNetLemmatizer()\n",
      "lemmas = [wnl.lemmatize(t) for t in tokens]\n",
      "print \"Lemma\\t\\tStemmed\\t\\tOriginal\",t\n",
      "for l,s,t in zip(lemmas[100:200], stemmed[100:200],tokens[100:200]):\n",
      "    print l,\"\\t\\t\",s,\"\\t\\t\",t"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource u'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/mgrani/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-3c05bc131dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#WordNet is a thesaurus providing this kind of information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Lemma\\t\\tStemmed\\t\\tOriginal\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/stem/wordnet.pyc\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/mgrani/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Step 3b: Tagging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Tagging refers to the process of adding (semantic) information to a token for later use. For example, a retrieval engine is not interested in so called \"Stopwords\" (and, or, it) and want to tag tokens falling into the stop word category.\n",
      "\n",
      "The most common tagging is \"Part of speech\" tagging, where every word is assigned to its word class/lexical category. This is a prerquisit for more complex analysis like information extraction. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#python example.\n",
      "# note that for using the tagger you need to download the maxent_treebank_pos_tagger  model  using nltk.download()\n",
      "text = nltk.word_tokenize(\"And now for something completely different\")\n",
      "nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[('And', 'CC'),\n",
        " ('now', 'RB'),\n",
        " ('for', 'IN'),\n",
        " ('something', 'NN'),\n",
        " ('completely', 'RB'),\n",
        " ('different', 'JJ')]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#an example with a homonym: equal reprenstation (homo), but different meaning \n",
      "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
      "nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[('They', 'PRP'),\n",
        " ('refuse', 'VBP'),\n",
        " ('to', 'TO'),\n",
        " ('permit', 'VB'),\n",
        " ('us', 'PRP'),\n",
        " ('to', 'TO'),\n",
        " ('obtain', 'VB'),\n",
        " ('the', 'DT'),\n",
        " ('refuse', 'NN'),\n",
        " ('permit', 'NN')]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assigned tags follow often some linugistic standards. In english it is usually the [Penn-Treebank Tag set](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and in German the [Stuttgart Tuebingen Tag Set (STTS)](http://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/TagSets/stts-table.html).\n",
      "\n",
      "The used tag set usually depends on the corpora on which a part-of-speech tagger has been build from (more on corporas later). Although most tag sets provide higher complexity, the following basic classes can be distinguished universally:\n",
      "\n",
      "\n",
      "\n",
      "|Tag \t|Meaning \t|English Examples|\n",
      "|-|-|-|\n",
      "|ADJ \t|adjective \t|new, good, high, special, big, local|\n",
      "|ADP \t|adposition \t|on, of, at, with, by, into, under|\n",
      "|ADV \t|adverb \t|really, already, still, early, now|\n",
      "|CONJ \t|conjunction \t|and, or, but, if, while, although|\n",
      "|DET \t|determiner, article \t|the, a, some, most, every, no, which|\n",
      "|NOUN \t|noun \t|year, home, costs, time, Africa|\n",
      "|NUM \t|numeral \t|twenty-four, fourth, 1991, 14:24|\n",
      "|PRT \t|particle \t|at, on, out, over per, that, up, with|\n",
      "|PRON \t|pronoun \t|he, their, her, its, my, I, us|\n",
      "|VERB \t|verb \t|is, say, told, given, playing, would|\n",
      "|. \t|punctuation |marks \t. , ; !|\n",
      "|X \t|other \t|ersatz, esprit, dunno, gr8, univeristy|"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Step 4: Extracting the Vocabulary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Extracting the vocabulary is relevant in order to obtain statistical information on word usage, to build up indices for search engines or for corpus linguistic studies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#creating a vocabulary is simple\n",
      "vocab_tokens = sorted(set(tokens))\n",
      "vocab_stemmed = sorted(set(stemmed))\n",
      "print len(vocab_tokens), \" vs. stemmed \",len(vocab_stemmed)\n",
      "print \"Reduction through stemming: %f %%\"%(100.0-len(vocab_stemmed)*100.0/len(vocab_tokens))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3915  vs. stemmed  3287\n",
        "Reduction through stemming: 16.040868 %\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "vocab = Counter(tokens)\n",
      "\n",
      "print \"\\nCommon tokens\"\n",
      "print vocab.most_common(40)\n",
      "vocab = Counter(stemmed)\n",
      "\n",
      "print \"\\nCommon stemmed tokens\"\n",
      "print vocab.most_common(40)\n",
      "\n",
      "print \"\\nMost different words\"\n",
      "diff = Counter(stemmed)\n",
      "diff.subtract(Counter(tokens))\n",
      "print diff.most_common(20)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Common tokens\n",
        "[(u',', 1850), (u\"'\", 1244), (u'>', 992), (u'the', 790), (u'.', 741), (u'(', 719), (u')', 718), (u'a', 505), (u'of', 436), (u'to', 398), (u'and', 326), (u':', 324), (u'in', 287), (u'[', 244), (u']', 244), (u'is', 207), (u'we', 203), (u'for', 190), (u\"''\", 183), (u'that', 171), (u'=', 163), (u'text', 154), (u'``', 129), (u'can', 127), (u';', 125), (u'string', 119), (u'words', 119), (u'as', 118), (u'this', 107), (u'with', 103), (u'word', 102), (u'are', 101), (u'it', 89), (u'The', 88), (u'you', 83), (u'using', 82), (u'...', 82), (u'or', 81), (u'from', 80), (u\"'s\", 79)]\n",
        "\n",
        "Common stemmed tokens\n",
        "[(u',', 1850), (u\"'\", 1244), (u'>', 992), (u'the', 790), (u'.', 741), (u'(', 719), (u')', 718), (u'a', 505), (u'of', 436), (u'to', 398), (u'and', 326), (u':', 324), (u'in', 287), (u'[', 244), (u']', 244), (u'word', 221), (u'is', 207), (u'we', 203), (u'for', 190), (u'use', 188), (u\"''\", 183), (u'string', 183), (u'that', 171), (u'text', 170), (u'=', 163), (u'``', 129), (u'can', 127), (u';', 125), (u'it', 120), (u'as', 118), (u'charact', 116), (u'thi', 107), (u'with', 103), (u'are', 101), (u'The', 88), (u'express', 87), (u'be', 84), (u'you', 83), (u'...', 82), (u'or', 81)]\n",
        "\n",
        "Most different words\n",
        "[(u'word', 119), (u'use', 117), (u'charact', 116), (u'thi', 107), (u'express', 87), (u'token', 70), (u'string', 64), (u'exampl', 48), (u'follow', 43), (u'ani', 42), (u'includ', 39), (u'Unicod', 33), (u'Thi', 32), (u'it', 31), (u'match', 29), (u'monti', 28), (u'contain', 26), (u'sequenc', 25), (u're.findal', 24), (u'oper', 23)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Exercise A - Preprocessing the Reuters Corpus with NLTK"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Please conduct Exercise A in [Exercises - Natural Language Processing with NLTK](exercise/III.NLTK-Intro-Exercise.ipynb)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false
     },
     "source": [
      "Analysing Text using NLTK"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false
     },
     "source": [
      "Information Extraction"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false
     },
     "source": [
      "Sentiment Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Corpora"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<div class=\"alert alert-warning\">\n",
      "A **text corpus** is a large and structured set of texts for conducting statistical analysis and to train/test models in NLP.\n",
      "</div>\n",
      "\n",
      "- **Annotated Corpora:** Text corporas where elements are annotated (e.g. Part-of-Speech tagging)\n",
      "- **Monolingual Corpora:** Corpora of one language\n",
      "- **Multilingual Corpora:** Corpora containing more than one language\n",
      "\n",
      "\n",
      "Corpora are the main subject of research in the field of Corpus Linguistics.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Available Corpora in NLTK"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "NLTK provides a \n",
      "\n",
      "- set of available corpora (annotated, mono and multilingual)\n",
      "- functions for loading, managing, storing and analysing corpora"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "<div class=\"alert alert-info\">\n",
      "Most NLTK functions do not require text to be represented as corpora. They can be applied on plain strings also. However, corpora abstract the management of strings and the associated information.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Further details at the [NLTK Corpora HOWTO](http://www.nltk.org/howto/corpus.html)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Types of Corpora"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Corporas usually contain a list of files, consisting of a list of structure elements (e.g. paragraphs, sentences) which consist of a list of words.\n",
      "\n",
      "Corpora and functions over corporae are provided via a so called corpus reader. The available functions depend on the type of corpora."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "NLTK supports different types of corpora, namely\n",
      "\n",
      "- **Plaintext Corpora**: Usually a set of files where every file contains text.\n",
      "- **Tagged Corpora**: Words are tagged with type information like for example part-of-speech for a word\n",
      "- **Chunked Corpora**: Corpora that provide chunk structures (as flat trees), like for example word, tag pairs that for a phrase\n",
      "- **Parsed Corpora**: Corpora that provide a full parsing tree\n",
      "- **Word Lists, Lexicons, Semantic Networks **: Word lookup tables forming external information on words/phrases (E.g. stopwords, forenames, synonyms). \n",
      "   - WordNet - an ontology/thesaurus containing semantic information on words like synonyms\n",
      "- **Categorized Corpora**: corpora where every document is assigned to a category"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.corpus as corpus"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Using Plain Text Corpora "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#lets take a look at the genesis corpus\n",
      "print \"The Genesis\"\n",
      "print \"Files: \",corpus.genesis.fileids()\n",
      "print \"Encoding of file %s is %s\"%(corpus.genesis.fileids()[0],\n",
      "                                   corpus.genesis.encoding(corpus.genesis.fileids()[0]))\n",
      "print \"Description:\", corpus.genesis.readme()\n",
      "print \"Number of Sentences %d:\"%len(corpus.genesis.sents())\n",
      "print \"First sentence:\"\n",
      "print \"Number of words %d:\"%len(corpus.genesis.words())\n",
      "print \"Words:\", corpus.genesis.words()\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false
     },
     "source": [
      "Using Tagged Corpora"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "print \"Words\", brown.words()\n",
      "print\n",
      "print \"Tagged words\", brown.tagged_words()\n",
      "print\n",
      "print \"Sentences\", brown.tagged_sents()\n",
      "print\n",
      "print \"pragraphs\", brown.tagged_paras()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#printing all nouns\n",
      "print \"Nouns: \", [i for i in brown.tagged_words() if i[1] ==\"NP-TL\" or i[1] ==\"NN-TL\"  ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false
     },
     "source": [
      "Categorized Corpora"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import movie_reviews\n",
      "print \"Categories are:\", movie_reviews.categories()\n",
      "for cat in movie_reviews.categories(): \n",
      "    print \"%d files in category %s\" % (len(movie_reviews.fileids(\"neg\")),cat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Working with Corpora Reader"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A corpora reader abstracts the corpus and provids access to it."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Corpora support the following common methods (depending on the corpus type)\n",
      "\n",
      "- words(): list of str\n",
      "- sents(): list of (list of str)\n",
      "- paras(): list of (list of (list of str))\n",
      "- tagged_words(): list of (str,str) tuple\n",
      "- tagged_sents(): list of (list of (str,str))\n",
      "- tagged_paras(): list of (list of (list of (str,str)))\n",
      "- chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
      "- parsed_sents(): list of (Tree with str leaves)\n",
      "- parsed_paras(): list of (list of (Tree with str leaves))\n",
      "- xml(): A single xml ElementTree\n",
      "- raw(): str (unprocessed corpus contents)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Automatically Created Cropus Reader Instances"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Are created when importung the module `nltk.corpus`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.corpus.brown"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.corpus.genesis"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.corpus.treebank"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Creating New Corpus Reader Instances"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Most often you are in the situation to create your own corpus. Depending on the location and format your corpus is stored you can instantiate corpus reader instances directly. \n",
      "\n",
      "The easiest way is using a plaint text corpus on the hard disk."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " # Find the directory where the corpus lives.\n",
      "genesis_dir = nltk.data.find('corpora/genesis.zip').join('genesis/')\n",
      "# Create our custom sentence tokenizer.\n",
      "my_sent_tokenizer = nltk.RegexpTokenizer('[^.!?]+')\n",
      "# Create the new corpus reader object.\n",
      "my_genesis = nltk.corpus.PlaintextCorpusReader(genesis_dir, '.*\\.txt', sent_tokenizer=my_sent_tokenizer)\n",
      "# Use the new corpus reader object.\n",
      "print(my_genesis.sents('english-kjv.txt')[0]) \n",
      "['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n",
      " 'and', 'the', 'earth']"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course this might not be the most efficient variant, since all text is loaded in memory. Also, you might want to load textual data from a stream, which would require implementing a new corpus reader. \n",
      "\n",
      "We will not go into the details here, but you can read it up at the [NLTK Corpora HOWTO](http://www.nltk.org/howto/corpus.html)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "heading_collapsed": false,
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Basic Steps in Text Preprocessing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Stuff for presentation\n",
      "#loads section numbering https://github.com/ipython/ipython/wiki/Extensions-Index\n",
      "%reload_ext secnum\n",
      "%secnum"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}