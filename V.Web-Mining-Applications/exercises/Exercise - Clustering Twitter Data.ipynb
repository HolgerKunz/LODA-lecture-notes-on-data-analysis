{
 "metadata": {
  "name": "",
  "signature": "sha256:d8f81b3bb7fb4114c3083839fb55ae588677da5f8760660915426dcbd6d49550"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import base64\n",
      "\n",
      "class TwitterAppOnlyAPI:\n",
      "    \"\"\"\n",
      "    This class implements simple functions for accessing the twitter api.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, client_id = \"IhrU9UwHys1IBnVXh2tLt5dDW\",\n",
      "                       client_secret = \"H4kL6FHOJlZ0wiCHUhqD2u3FmRm2q8vKJRqoqCwJzEN5t9Fsfw\"):\n",
      "        self.client_id = client_id\n",
      "        self.client_secret = client_secret\n",
      "        self.access_token = self.__get_access_token();\n",
      "\n",
      "    def __get_access_token(self):\n",
      "        \"\"\"\n",
      "        This function uses the client secrete and client id in order to get an access token\n",
      "        \"\"\"\n",
      "        base64EncodedCredentials = base64.b64encode(self.client_id + \":\" + self.client_secret)\n",
      "        params = {\"grant_type\":\"client_credentials\"}\n",
      "        header = {\"Content-Type\":\"application/x-www-form-urlencoded;charset=UTF-8\",\n",
      "                  \"Authorization\":\"Basic \" + base64EncodedCredentials}\n",
      "\n",
      "        r = requests.post(\"https://api.twitter.com/oauth2/token\", \n",
      "                          data = params,\n",
      "                          headers = header)\n",
      "\n",
      "        if r.status_code == requests.codes.ok:\n",
      "            return r.json()[\"access_token\"]\n",
      "        else:\n",
      "            raise IOError(\"Could not access Twitter. Maybe invalid credentials?\")\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"a\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "a\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_tweets(tweets):\n",
      "    return [(s[\"text\"],s[\"user\"][\"screen_name\"],s[\"created_at\"]) for s in tweets]\n",
      "\n",
      "#lets do some monkey patching resp. duck punching\n",
      "#since it is very unusual to do this for a java developer\n",
      "#For more on duck punching/metaprogramming \n",
      "#see http://www.ericdelabar.com/2008/05/metaprogramming-javascript.html\n",
      "def search(self, query, count = 20):\n",
      "    params = {\"q\" : query,\n",
      "              \"count\" : count}             \n",
      "    header = {\"Authorization\" : \"Bearer {}\".format(self.access_token),\n",
      "              'Accept-Encoding': 'gzip',}\n",
      "    r = requests.get(\"https://api.twitter.com/1.1/search/tweets.json\",\n",
      "                 params = params,\n",
      "                 headers = header)\n",
      "    r.encoding = \"utf-8\"\n",
      "    return parse_tweets(r.json()[\"statuses\"])\n",
      "    \n",
      "TwitterAppOnlyAPI.search = search"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "api = TwitterAppOnlyAPI()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "from collections import Counter\n",
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "\n",
      "import logging\n",
      "import sys\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class ClusterAPI:\n",
      "    \n",
      "    def __init__(self, n_hashing_feature = 0, use_idf = False, stop_words='english'):\n",
      "        if n_hashing_feature > 0:\n",
      "            if use_idf:\n",
      "                # Perform an IDF normalization on the output of HashingVectorizer\n",
      "                hasher = HashingVectorizer(n_features=n_hashing_feature,\n",
      "                                           stop_words=stop_words, non_negative=True,\n",
      "                                           norm=None, binary=False)\n",
      "                self.vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
      "            else:\n",
      "                self.vectorizer = HashingVectorizer(n_features=n_hashing_feature,\n",
      "                                               stop_words=stop_words,\n",
      "                                               non_negative=False, norm='l2',\n",
      "                                               binary=False)\n",
      "        else:\n",
      "            self.vectorizer = TfidfVectorizer(max_df=0.5,\n",
      "                                         min_df=2, stop_words=stop_words,\n",
      "                                         use_idf=use_idf)\n",
      "            \n",
      "            \n",
      "    def transform(self,data):\n",
      "       return self.vectorizer.fit_transform(data)\n",
      "            \n",
      "    def project(self, n_components, data):        \n",
      "        # Vectorizer results are normalized, which makes KMeans behave as\n",
      "        # spherical k-means for better results. Since LSA/SVD results are\n",
      "        # not normalized, we have to redo the normalization.\n",
      "        svd = TruncatedSVD(opts.n_components)\n",
      "        lsa = make_pipeline(svd, Normalizer(copy=False))\n",
      "\n",
      "        X = lsa.fit_transform(data)\n",
      "\n",
      "\n",
      "        explained_variance = svd.explained_variance_ratio_.sum()\n",
      "        return X, explained_variance\n",
      "      \n",
      "            \n",
      "    def cluster(self, data, minibatch = True, k = 3, verbose = False):\n",
      "        if minibatch:\n",
      "            km = MiniBatchKMeans(n_clusters=k, init='k-means++', n_init=1,\n",
      "                         init_size=1000, batch_size=1000, verbose = verbose)\n",
      "        else:\n",
      "            km = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1,\n",
      "                        verbose=verbose)  \n",
      "        km.fit(data)\n",
      "        return km\n",
      "                \n",
      "    def top_terms(self, model):    \n",
      "        order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
      "        terms = self.vectorizer.get_feature_names()\n",
      "        cluster_terms = []\n",
      "\n",
      "        for i in range(len(model.cluster_centers_)):\n",
      "            cluster_terms.append([])\n",
      "            for ind in order_centroids[i, :10]:\n",
      "                cluster_terms[i].append(terms[ind])\n",
      "        return cluster_terms\n",
      "    \n",
      "    def print_summary(self, model, data):\n",
      "        tt = self.top_terms(model)\n",
      "        print \"Top Terms:\"\n",
      "        for t in tt:\n",
      "            print \"\\t\", t\n",
      "        print \"Document Distribution\", Counter(model.predict(data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = api.search(\"merkel\", count = 500)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster = ClusterAPI(use_idf = True)\n",
      "text = [t[0].encode(\"ascii\",\"ignore\") for t in data]\n",
      "vectors = cluster.transform(text)\n",
      "model = cluster.cluster(vectors, k=40)\n",
      "cluster.print_summary(model, vectors)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top Terms:\n",
        "\t[u'para', u'alto', u'poroshenko', u'fuego', u'reunirse', u'exige', u'desarme', u'putin', u'al', u'el']\n",
        "\t[u'ist', u'unsere', u'antisemitismus', u'staatliche', u'pflicht', u'brgerliche', u'und', u'vczl6piqel', u'nicokahl', u'die']\n",
        "\t[u'eine', u'auftreten', u'klarheit', u'kanzlerin', u'im', u'neue', u'gibt', u'pltzlich', u'spricht', u'klartext']\n",
        "\t[u'germany', u'aegis', u'patriots', u'split', u'german', u'islam', u'angela', u'galliawatch', u'g7', u'god']\n",
        "\t[u'government', u'president', u'gave', u'chancellor', u'approval', u'minefornothing', u'met', u'wednesday', u'spokesman', u'ecb']\n",
        "\t[u'faz_politik', u'joergforbrig', u'russia', u'render', u'moscow', u'groundless', u'remain', u'sanctions', u'hand', u'place']\n",
        "\t[u'olmalyd', u'alamasnn', u'nedeni', u'dn', u'sandmzdan', u'ama', u'durum', u'gibi', u'yanaa', u'yanak']\n",
        "\t[u'el', u'en', u'obama', u'la', u'https', u'ucrania', u'putin', u'se', u'del', u'kanzlerin']\n",
        "\t[u'sprechen', u'ukraine', u'erneut', u'ber', u'sputnik', u'sfxn4xm7hu', u'xdqceqae4a', u'putinizer', u'deutschland', u'und']\n",
        "\t[u'million', u'march', u'paris', u'bundeskanzlerin', u'die', u'galliawatch', u'gov', u'god', u'gibt', u'gibi']\n",
        "\t[u'zu', u'der', u'und', u'angela', u'gov', u'god', u'gibt', u'gibi', u'germany', u'germans']\n",
        "\t[u'https', u'nn', u'syriza', u'la', u'los', u'auf', u'von', u'parasdr', u'papa', u'paris']\n",
        "\t[u'gt', u'oyunu', u'bakan', u'mslmanlarla', u'devlet', u'birlikte', u'gstergesiydi', u'gauck', u'alman', u'grdklerinin']\n",
        "\t[u'retention', u'case', u'curious', u'data', u'ideas', u'eu', u'angela', u'https', u'zx31evvla8', u'gave']\n",
        "\t[u'afferma', u'portavoce', u'si', u'governo', u'bce', u'mercoled', u'sono', u'tedesco', u'incontrati', u'lo']\n",
        "\t[u'islam', u'der', u'gehrt', u'zu', u'deutschland', u'frau', u'nicht', u'fr', u'ist', u'https']\n",
        "\t[u'stopsharia', u'conquest', u'flpdyrirly', u'germany', u'islam', u'zx31evvla8', u'galliawatch', u'gibt', u'gibi', u'germans']\n",
        "\t[u'germans', u'rallies', u'away', u'arrest', u'urges', u'stay', u'frau', u'muslims', u'ok', u'pegida']\n",
        "\t[u'slam', u'dini', u'aittir', u'almanyaya', u'zx31evvla8', u'szlerinden', u'davutolunun', u'deitirdi', u'sonra', u'sweetnovemberrr']\n",
        "\t[u'angela', u'obama', u'zx31evvla8', u'galliawatch', u'gov', u'god', u'gibt', u'gibi', u'germany', u'germans']\n",
        "\t[u'mit', u'hat', u'zu', u'die', u'wie', u'auf', u'deutschland', u'exige', u'gauck', u'god']\n",
        "\t[u'response', u'women', u'march', u'million', u'german', u'zx31evvla8', u'g7', u'god', u'gibt', u'gibi']\n",
        "\t[u'drone', u'wants', u'distance', u'stop', u'chacellor', u'supporting', u'terr', u'terrorism', u'want', u'emran_feroz']\n",
        "\t[u'gov', u'said', u'planned', u'spox', u'wed', u'unaware', u'yannikouts', u'meeting', u'wednesday', u'met']\n",
        "\t[u'informal', u'meetings', u'regularly', u'meeting', u'place', u'spokesman', u'ecb', u'draghi', u'zx31evvla8', u'galliawatch']\n",
        "\t[u'grecia', u'euro', u'siga', u'quiere', u'pero', u'que', u'el', u'en', u'se', u'al']\n",
        "\t[u'renzi', u'da', u'zx31evvla8', u'g7', u'god', u'gibt', u'gibi', u'germany', u'germans', u'german']\n",
        "\t[u'fr', u'wie', u'syriza', u'und', u'zx31evvla8', u'galliawatch', u'god', u'gibt', u'gibi', u'germany']\n",
        "\t[u'claim', u'pegidas', u'ujn2kvhq9w', u'islamization', u'challenges', u'https', u'zx31evvla8', u'gave', u'god', u'gibt']\n",
        "\t[u'russian', u'receive', u'invite', u'boots', u'leader', u'says', u'putin', u'angela', u'germany', u'germans']\n",
        "\t[u'god', u'billbrowder', u'thank', u'russia', u'groundless', u'sanctions', u'render', u'remain', u'hand', u'moscow']\n",
        "\t[u'g7', u'chance', u'summit', u'invited', u'says', u'putin', u'germany', u'gibt', u'gibi', u'germans']\n",
        "\t[u'mslmanlar', u'parasdr', u'papa', u'bir', u'ok', u'en', u'zx31evvla8', u'gave', u'god', u'gibt']\n",
        "\t[u'eu', u'zx31evvla8', u'galliawatch', u'gov', u'god', u'gibt', u'gibi', u'germany', u'germans', u'german']\n",
        "\t[u'die', u'zx31evvla8', u'g7', u'gov', u'god', u'gibt', u'gibi', u'germany', u'germans', u'german']\n",
        "\t[u'mario', u'ecb', u'zx31evvla8', u'g7', u'god', u'gibt', u'gibi', u'germany', u'germans', u'german']\n",
        "\t[u'ucrania', u'hollande', u'zx31evvla8', u'government', u'god', u'gibt', u'gibi', u'germany', u'germans', u'german']\n",
        "\t[u'que', u'lo', u'es', u'en', u'el', u'angela', u'kanzlerin', u'fuego', u'gibi', u'germany']\n",
        "\t[u'en', u'los', u'es', u'leader', u'g7', u'god', u'gibt', u'gibi', u'germany', u'germans']\n",
        "\t[u'nicht', u'von', u'zx31evvla8', u'governo', u'gov', u'god', u'gibt', u'gibi', u'germany', u'germans']\n",
        "Document Distribution Counter({11: 12, 2: 5, 4: 5, 0: 4, 1: 4, 15: 4, 3: 3, 7: 3, 8: 3, 12: 3, 14: 3, 18: 3, 19: 3, 5: 2, 6: 2, 9: 2, 13: 2, 16: 2, 17: 2, 20: 2, 21: 2, 22: 2, 23: 2, 24: 2, 25: 2, 26: 2, 28: 2, 29: 2, 30: 2, 31: 2, 32: 2, 10: 1, 27: 1, 33: 1, 34: 1, 35: 1, 36: 1, 37: 1, 38: 1, 39: 1})\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clusters = model.predict(vectors)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.asarray(data)[clusters==16]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "array([[ u'RT @rightsidenews: ISLAM CONQUEST OF GERMANY #stopsharia http://t.co/FLpdyrIrLY',\n",
        "        u'HKeithbuckley', u'Fri Jan 16 14:25:01 +0000 2015'],\n",
        "       [u'ISLAM CONQUEST OF GERMANY #stopsharia http://t.co/FLpdyrIrLY',\n",
        "        u'rightsidenews', u'Fri Jan 16 14:21:56 +0000 2015']], \n",
        "      dtype='<U144')"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}